# -*- coding: utf-8 -*-
"""improvedARIMA_Nov15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RvaqjAPgGj72cUMYy4ITOEXAr4IbeqV_
"""

import nbconvert
import os            ##  This module is for "operating system" interfaces
import sys           ##  This module is for functionality relevant to the python run time
import numpy as np
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
GOOGLE_DRIVE_PATH = os.path.join('drive','My Drive', '/content/drive/MyDrive/Colab Notebooks/PhD research/')

sys.path.append(GOOGLE_DRIVE_PATH)
os.chdir(GOOGLE_DRIVE_PATH)

"""# Here, we explore various model fits to determine which one best captures the underlying patterns and behavior of our flow data. GEV was identified as the most appropriate model that accurately represents the dynamics of the flow based on Q-Q plots and KS statistic"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.stats import genpareto, weibull_min, lognorm, genextreme
from statsmodels.distributions.empirical_distribution import ECDF

data = pd.read_csv('plant_flow_data.csv')
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)
flow_data = data['flow']

# this is a core portion of our analysis. We begin with defining a function that can be used to fit and evaluate various distributions of our flow data.
def fit_distribution(data, dist, name):
    params = dist.fit(data)
    ks_stat, p_value = stats.kstest(data, dist.name, args=params)
    print(f"{name} - KS Statistic: {ks_stat:.4f}, p-value: {p_value:.4f}")
    return params, ks_stat, p_value

# tested out list of distributions

distributions = {
    "Generalized Pareto": genpareto,
    "Weibull": weibull_min,
    "Log-normal": lognorm,
    "Generalized Extreme Value": genextreme
}

results = {}

# In this step, we fit each candidate distribution to the flow data and systematically store the corresponding parameters and the results of the Kolmogorov-Smirnov (KS) test.
# The function will apply various probability distributions to the data, estimate the best-fit parameters for each distribution, and assess the goodness of fit using the KS statistic.
for name, dist in distributions.items():
    print(f"Fitting {name} distribution...")
    params, ks_stat, p_value = fit_distribution(flow_data, dist, name)
    results[name] = {
        "params": params,
        "ks_stat": ks_stat,
        "p_value": p_value
    }

# The Q-Q plot function visually assesses how well a particular distribution fits the flow data.
#function compares the quantiles of the observed data to the quantiles of a specified theoretical distribution
def qq_plot(data, dist, params, name):
    sorted_data = np.sort(data)
    quantiles = np.linspace(0, 1, len(sorted_data))
    theoretical_quantiles = dist.ppf(quantiles, *params)

    plt.figure(figsize=(8, 6))
    plt.scatter(theoretical_quantiles, sorted_data, color='blue', label=f'Data vs {name}')
    plt.plot([sorted_data.min(), sorted_data.max()], [sorted_data.min(), sorted_data.max()], 'r--', label='45-degree line')
    plt.xlabel(f'Theoretical Quantiles ({name})')
    plt.ylabel('Observed Quantiles')
    plt.title(f'Q-Q Plot for {name}')
    plt.legend()
    plt.grid(False)
    plt.show()

for name, result in results.items():
    qq_plot(flow_data, distributions[name], result["params"], name)

#KS statistics come here
best_fit = min(results.items(), key=lambda x: x[1]["ks_stat"])
print("\nBest-fitting distribution based on KS Statistic:")
print(f"{best_fit[0]} - KS Statistic: {best_fit[1]['ks_stat']:.4f}, p-value: {best_fit[1]['p_value']:.4f}")

"""# since GEV has been identified as the most appropriate, we now proceed to determine the low and high flow thresholds based on its cumulative distribution function (CDF). The CDF of the GEV distribution provides a framework for understanding the probability that the flow data will fall below a given threshold, allowing us to identify the critical low and high flow values that are significant for analysis and decision-making. To determine the low and high flow thresholds, we first specify percentile cutoffs, such as the 10th percentile for low flow and the 90th percentile for high flow. Using the CDF of the fitted GEV distribution, we calculate the flow values corresponding to these cumulative probabilities. These values represent the thresholds beyond which we expect extreme low or high flow to occur with a specified probability. the shape parameter is negative here??"""

def fit_gev_distribution(data):
    params = genextreme.fit(data)
    return params

gev_params = fit_gev_distribution(flow_data)
xi, loc, scale = gev_params

print(f"GEV Shape Parameter (xi): {xi:.4f}")
print(f"GEV Location Parameter (loc): {loc:.4f}")
print(f"GEV Scale Parameter (scale): {scale:.4f}")

# Independent CDF function based on GEV
def independent_cdf(y, xi, loc, scale):
    return genextreme.cdf(y, xi, loc=loc, scale=scale)

#low, normal, and high flow based on empirical CDF
low_flow_threshold_empirical = np.percentile(flow_data, 10)
high_flow_threshold_empirical = np.percentile(flow_data, 90)

#low, normal, and high flow based on GEV CDF
flow_values = np.linspace(flow_data.min(), flow_data.max(), 100)
cdf_values = independent_cdf(flow_values, xi, loc, scale)
low_flow_threshold_gev = flow_values[np.argmax(cdf_values >= 0.10)]
high_flow_threshold_gev = flow_values[np.argmax(cdf_values >= 0.90)]

# GEV CDF
plt.figure(figsize=(12, 6))
plt.plot(flow_values, cdf_values, label='Independent GEV CDF', color='blue')
plt.axvline(low_flow_threshold_gev, color='orange', linestyle='--', label=f'GEV low flow : {low_flow_threshold_gev:.2f}')
plt.axvline(high_flow_threshold_gev, color='red', linestyle='--', label=f'GEV high flow: {high_flow_threshold_gev:.2f}')

# Empirical CDF
sorted_flow_data = np.sort(flow_data)
empirical_cdf = np.arange(1, len(sorted_flow_data) + 1) / len(sorted_flow_data)
plt.step(sorted_flow_data, empirical_cdf, label='Empirical CDF', color='green', where='post')
plt.axvline(low_flow_threshold_empirical, color='purple', linestyle='--', label=f'Empirical low flow: {low_flow_threshold_empirical:.2f}')
plt.axvline(high_flow_threshold_empirical, color='brown', linestyle='--', label=f'Empirical high flow: {high_flow_threshold_empirical:.2f}')
plt.fill_betweenx([0, 1], low_flow_threshold_gev, high_flow_threshold_gev, color='lightblue', alpha=0.5, label='Normal flow range (GEV)')
plt.fill_betweenx([0, 1], low_flow_threshold_empirical, high_flow_threshold_empirical, color='lightgray', alpha=0.5, label='Normal flow range (Empirical)')
plt.title('Flow Categories Based on GEV CDF')
plt.xlabel('Flow (million gallons per day)')
plt.ylabel('Cumulative Flow Distribution')
plt.legend()
plt.grid(False)
plt.show()

print(f"Empirical low flow (10th Percentile): {low_flow_threshold_empirical:.2f}")
print(f"Empirical high flow (90th Percentile): {high_flow_threshold_empirical:.2f}")
print(f"GEV low flow (10th Percentile): {low_flow_threshold_gev:.2f}")
print(f"GEV high flow (90th Percentile): {high_flow_threshold_gev:.2f}")

"""# so we explore various statistical models to identify the best fit for our rainfall data. A key characteristic of our dataset is the high prevalence of zero valuesâ€”approximately 65.85% of the data consists of zero rainfall events. This unique feature significantly impacts the choice of appropriate models, as standard distributions may not adequately capture the large proportion of zeroes. Given the abundance of zero values, we must focus on specialized models designed to handle this kind of data, such as hurdle models and zero-inflated models, which explicitly account for excess zero values in the dataset. These models provide a more accurate representation by distinguishing between two processes: one that generates zero values and another that models the non-zero rainfall amounts. The goal is to explore and compare these models, assessing their ability to fit the data while properly accounting for the zero-inflation. several candidate models are considered, each of which incorporates mechanisms for handling the abundance of zeros."""

rainfall_data = data['rainfall']
zero_count = np.sum(rainfall_data == 0)
total_count = len(rainfall_data)
zero_percentage = (zero_count / total_count) * 100

print(f"Percentage of no rainfall instances: {zero_percentage:.2f}%")

"""*The Hurdle-Gamma model appeared to provide the best fit for rainfall, as it a low KS statistics and p-value was reasonable (just under 0.05). However, these values suggested that they are not perfect fits either, but are much better than other models, especially the Gamma and Weibull models*"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from statsmodels.distributions.empirical_distribution import ECDF

rainfall_data = data['rainfall']

# here we are fitting a Hurdle-Gamma and Hurdle-LogNormal models
def fit_hurdle_model(data, distribution):
    zero_count = np.sum(data == 0)
    total_count = len(data)
    p0 = zero_count / total_count  # probability of no rain
    non_zero_data = data[data > 0]
    p1 = 1 - p0  # this is the probability of rain

    # fit the distribution to non-zero data separately here
    if distribution == 'gamma' and len(non_zero_data) > 0:
        shape, loc, scale = stats.gamma.fit(non_zero_data, floc=0)
        dist_params = (shape, loc, scale)
    elif distribution == 'lognormal' and len(non_zero_data) > 0:
        shape, loc, scale = stats.lognorm.fit(non_zero_data, floc=0)
        dist_params = (shape, loc, scale)
    else:
        dist_params = (0, 1)  # check when there is no non-zero data

    return p0, p1, dist_params

# lets define the Zero-Inflated Poisson (ZIP) model
def fit_zip_model(data):
    zero_count = np.sum(data == 0)
    total_count = len(data)
    p0 = zero_count / total_count
    p1 = 1 - p0
    mean_non_zero = np.mean(data[data > 0])
    return p0, p1, mean_non_zero

# we define the Zero-Inflated Negative Binomial (ZINB) model
def fit_zinb_model(data):
    zero_count = np.sum(data == 0)
    total_count = len(data)
    p0 = zero_count / total_count
    p1 = 1 - p0
    non_zero_data = data[data > 0]

    if len(non_zero_data) > 0:
        mu = np.mean(non_zero_data)
        var = np.var(non_zero_data)
        size = mu**2 / (var - mu) if var > mu else 1
    else:
        size = 1  # Default size if no non-zero data

    return p0, p1, size

# CDF of the Hurdle models
def compute_hurdle_cdf(y, p0, p1, dist_params, dist_type):
    if y < 0:
        return 0
    elif y == 0:
        return p0
    else:
        if dist_type == 'gamma':
            return p0 + p1 * stats.gamma.cdf(y, *dist_params)
        elif dist_type == 'lognormal':
            return p0 + p1 * stats.lognorm.cdf(y, *dist_params)
        return p0

# save the results for these different models here
models = {
    'Hurdle-Gamma': fit_hurdle_model(rainfall_data, 'gamma'),
    'Hurdle-LogNormal': fit_hurdle_model(rainfall_data, 'lognormal'),
    'ZIP': fit_zip_model(rainfall_data),
    'ZINB': fit_zinb_model(rainfall_data),
    'Gamma': stats.gamma.fit(rainfall_data[rainfall_data > 0], floc=0),
    'Weibull': stats.weibull_min.fit(rainfall_data[rainfall_data > 0])
}

# Initialize the Empirical CDF
y_values = np.sort(rainfall_data)
empirical_cdf = ECDF(rainfall_data)(y_values)
ks_results = {}

# Calculate KS statistic and p-value for each model
for model_name, params in models.items():
    if model_name.startswith('Hurdle'):
        p0, p1, dist_params = params
        dist_type = 'gamma' if 'Gamma' in model_name else 'lognormal'
        cdf_values = [compute_hurdle_cdf(y, p0, p1, dist_params, dist_type) for y in y_values]
    elif model_name == 'ZIP':
        p0, p1, mean_non_zero = params
        cdf_values = [p0 if y == 0 else p0 + p1 * stats.poisson.cdf(y, mean_non_zero) for y in y_values]
    elif model_name == 'ZINB':
        p0, p1, size = params
        cdf_values = [p0 if y == 0 else p0 + p1 * stats.nbinom.cdf(y, n=size, p=0.5) for y in y_values]
    else:
        dist_func = stats.gamma if model_name == 'Gamma' else stats.weibull_min
        cdf_values = dist_func.cdf(y_values, *params)

    # Calculate KS statistic
    ks_statistic = np.max(np.abs(empirical_cdf - cdf_values))

    # simulating data from the fitted distribution is to assess how well the model captures the underlying distribution of our observed rainfall data.
    # this generation of new data points using the estimated distribution parameters, we can compare the simulated data to the actual observed data and quantify how well the model fits.
    if model_name == 'Gamma':
        simulated_data = np.random.gamma(params[0], scale=params[2], size=1000)  # params[0]: shape, params[2]: scale
    elif model_name == 'Weibull':
        simulated_data = stats.weibull_min.rvs(*params, size=1000)
    elif model_name.startswith('Hurdle'):
        if dist_type == 'gamma':
            simulated_data = stats.gamma.rvs(*dist_params, size=1000)
        elif dist_type == 'lognormal':
            simulated_data = stats.lognorm.rvs(*dist_params, size=1000)
    else:
        # here if it's neither a hurdle or standard model, use non-zero rainfall data for KS comparison
        simulated_data = rainfall_data[rainfall_data > 0]

    # Compute KS test p-value
    ks_pvalue = stats.ks_2samp(rainfall_data[rainfall_data > 0], simulated_data)[1]
    ks_results[model_name] = (ks_statistic, ks_pvalue)

# Display KS results
for model_name, (ks_statistic, ks_pvalue) in ks_results.items():
    print(f"{model_name} - KS Statistic: {ks_statistic:.4f}, p-value: {ks_pvalue:.4f}")

"""# ARIMA - here we determine if our given time series (target and predictors) are stationary - all variables for ARIMA were found to be stationary. so no need to transform them but found out p, d,q automatically"""

import pandas as pd
from statsmodels.tsa.stattools import adfuller
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
data['season'] = label_encoder.fit_transform(data['season']) # we have to make the season variable numerical

def adf_test_all(data, columns):
    for column in columns:
        result = adfuller(data[column])
        print(f'ADF Statistic for {column}: {result[0]}')
        print(f'p-value: {result[1]}')
        print('Critical Values:')
        for key, value in result[4].items():
            print(f'\t{key}: {value}')
        print()

our_data = ['flow', 'rainfall', 'season', 'ADD']

adf_test_all(data, our_data)

"""# ARIMA model only"""

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plt.figure(figsize=(12, 10))

# ACF for flow
plt.subplot(3, 2, 1)
plot_acf(data['flow'], lags=20, ax=plt.gca())
plt.title('ACF for Flow')

# PACF for flow
plt.subplot(3, 2, 2)
plot_pacf(data['flow'], lags=20, ax=plt.gca())
plt.title('PACF for Flow')

# ACF for rainfall
plt.subplot(3, 2, 3)
plot_acf(data['rainfall'], lags=20, ax=plt.gca())
plt.title('ACF for Rainfall')

# PACF for rainfall
plt.subplot(3, 2, 4)
plot_pacf(data['rainfall'], lags=20, ax=plt.gca())
plt.title('PACF for Rainfall')

# ACF for ADD
plt.subplot(3, 2, 5)
plot_acf(data['ADD'], lags=20, ax=plt.gca())
plt.title('ACF for ADD')

# PACF for ADD
plt.subplot(3, 2, 6)
plot_pacf(data['ADD'], lags=20, ax=plt.gca())
plt.title('PACF for ADD')

plt.tight_layout()
plt.show()

"""# our first ARIMA model will account for cyclical patterns and potential anomalies by incorporating both temporal (day of the week, holidays, weekend indicators) and environmental factors (e.g., rolling averages of flow)"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from math import sqrt
import holidays

# Ensure that this is daily data
data = data.asfreq('D')

# Feature engineering
data['flow_rolling_mean_7'] = data['flow'].rolling(window=7).mean()  # 7-day rolling mean for flow
data['flow_rolling_std_7'] = data['flow'].rolling(window=7).std()  # 7-day rolling std for flow is used here to capture large fluctuations in flow over time. the model can use this to understand the periods of high or low volatility
data['WOY'] = data.index.isocalendar().week  # Week of the year
data['flow_lag1'] = data['flow'].shift(1)  # Adding lagged flow as a feature
data['flow_lag2'] = data['flow'].shift(2)  # Adding lagged flow as a feature
# Holidays based on U.S calendar
us_holidays = holidays.US(years=data.index.year.unique())
data['holiday'] = data.index.to_series().apply(lambda x: 1 if x in us_holidays else 0)
# we define the rolling averages for rainfall from 2-day to 10-day windows
for n in range(2, 11):
    data[f'rainfall_rolling_avg_{n}'] = data['rainfall'].rolling(window=n).mean()

# Drop missing values
data.dropna(inplace=True)
# (X) define the predictors and (y) the target
X = data[['rainfall', 'ADD', 'season', 'flow_lag1', 'flow_lag2',
          'flow_rolling_mean_7', 'flow_rolling_std_7', 'WOY', 'holiday'] +
         [f'rainfall_rolling_avg_{n}' for n in range(2, 11)]]
y = data['flow'].values

# Data splitting
size = int(len(y) * 0.8)  # 80% for training, 20% for testing
train, test = y[:size], y[size:]
X_train, X_test = X[:size], X[size:]

# Preprocessing (scaling numerical and encoding categorical features)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['rainfall', 'ADD', 'flow_lag2',
                                   'flow_rolling_mean_7', 'flow_rolling_std_7', 'flow_lag1', 'WOY'] +
                                  [f'rainfall_rolling_avg_{n}' for n in range(2, 11)]),  # Add the rolling features
        ('cat', OneHotEncoder(drop='first'), ['season', 'holiday'])  # OneHotEncode 'season' and 'holiday'
    ])

X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

# ARIMA model
p, d, q = 1, 0, 1  # ARIMA parameters, visually inspected PACF and ACF
model = ARIMA(train, order=(p, d, q), exog=X_train_scaled)
model_fit = model.fit()

# Make predictions
predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, exog=X_test_scaled)

# Model performance metrics
rmse = sqrt(mean_squared_error(test, predictions))
mae = mean_absolute_error(test, predictions)
r2 = r2_score(test, predictions)

print(f'RMSE: {rmse:.2f}, MAE: {mae:.2f}, RÂ²: {r2:.2f}')

# Plot Observed vs. Predicted
plt.figure(figsize=(12, 6))
dates_test = data.index[size:]  # Corresponding dates for the test set
dates_pred = data.index[size:len(train) + len(predictions)]  # Dates for predictions

plt.plot(dates_test, test, label='Observed', color='blue')
plt.plot(dates_pred, predictions, label='Predicted', color='red')
plt.title('ARIMAX')
plt.xlabel('Date')
plt.ylabel('Flow (million gallons per day)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# so we now explore this model further based on additional elements i.e.  extreme value analysis from what we learned about the flow and rainfall data distribution. GEV is used here to model flow data, which helps in characterizing extreme events. Assigning adaptive weights to extreme values, we emphasize high-impact, rare events in the model training phase. However, ARIMA does not inherently support such custom weighting based on the probability of extreme events, making this approach somewhat unique in general ARIMAX implementation

"""

from scipy.stats import gamma

def fit_hurdle_gamma_model(data):
    # Probability of no rain (p0)
    zero_count = np.sum(data == 0)
    total_count = len(data)
    p0 = zero_count / total_count  # probability of no rain
    p1 = 1 - p0  # probability of rain

    # Non-zero data (rainfall > 0)
    non_zero_data = data[data > 0]

    # Fit Gamma distribution to non-zero data
    shape, loc, scale = gamma.fit(non_zero_data, floc=0)  # fix loc=0 for standard Gamma

    # Return parameters
    return p0, p1, (shape, loc, scale)
rainfall = data['rainfall']
p0_rainfall, p1_rainfall, dist_params_rainfall = fit_hurdle_gamma_model(rainfall)

# Calculate the weights for extreme rainfall events
def calculate_rainfall_weights(data, p0, p1, dist_params, threshold, extreme_penalty=1.0):
    weights = np.ones(len(data))
    exceedances = data > threshold
    weights[exceedances] = gamma.pdf(data[exceedances], *dist_params)
    weights = weights / np.max(weights)
    extreme_threshold = np.percentile(data, 90)
    weights[data >= extreme_threshold] *= extreme_penalty
    return weights

# Calculate weights for the rainfall data
rainfall_threshold = np.percentile(rainfall, 90)  # Define threshold for extreme rainfall
weights_rainfall = calculate_rainfall_weights(rainfall, p0_rainfall, p1_rainfall, dist_params_rainfall, rainfall_threshold, extreme_penalty=2.0)

# GEV Model for Flow (Extreme Flow Events)
def fit_gev_distribution(data):
    params = genextreme.fit(data)
    return params

# Assuming target_flow is the variable that holds the flow data
flow = data['flow']
target_flow = flow[flow > 0]  # Non-zero flow data
gev_params = fit_gev_distribution(target_flow)  # GEV for non-zero flow data
xi, loc, scale = gev_params

# Calculate the weights for extreme flow events
def calculate_flow_weights(data, xi, loc, scale, threshold, extreme_penalty=1.0):
    weights = np.ones(len(data))
    exceedances = data > threshold
    weights[exceedances] = genextreme.pdf(data[exceedances], xi, loc, scale)
    weights = weights / np.max(weights)
    extreme_threshold = np.percentile(data, 90)
    weights[data >= extreme_threshold] *= extreme_penalty
    return weights

# Calculate weights for the flow data
flow_threshold = np.percentile(target_flow, 95)  # Define threshold for extreme flow
weights_flow = calculate_flow_weights(target_flow, xi, loc, scale, flow_threshold, extreme_penalty=2.0)

# Add these weights as features to your data
data['weights_rainfall'] = weights_rainfall
data['weights_flow'] = weights_flow

# Feature Engineering for the Model
data['flow_rolling_mean_7'] = data['flow'].rolling(window=7).mean()  # 7-day rolling mean for flow
data['flow_rolling_std_7'] = data['flow'].rolling(window=7).std()  # 7-day rolling std for flow
data['WOY'] = data.index.isocalendar().week  # Week of the year
for n in range(2, 11):
    data[f'rainfall_rolling_avg_{n}'] = data['rainfall'].rolling(window=n).mean()
data['flow_lag1'] = data['flow'].shift(1)  # Adding lagged flow as a feature
data['flow_lag2'] = data['flow'].shift(2)  # Adding lagged flow as a feature
data.dropna(inplace=True)

# holidays based on U.S calendar
us_holidays = holidays.US(years=data.index.year.unique())
data['holiday'] = data.index.to_series().apply(lambda x: 1 if x in us_holidays else 0)

# (X) define the predictors and (y) the target
X = data[['rainfall', 'ADD', 'season', 'flow_lag1', 'flow_lag2', 'weights_rainfall', 'weights_flow',
          'flow_rolling_mean_7', 'flow_rolling_std_7', 'WOY', 'holiday'] +
         [f'rainfall_rolling_avg_{n}' for n in range(2, 11)]]
y = data['flow'].values

# Data splitting
size = int(len(y) * 0.8)
train, test = y[:size], y[size:]
X_train, X_test = X[:size], X[size:]

# Preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['rainfall', 'ADD', 'flow_lag2', 'flow_rolling_mean_7',
                                   'flow_rolling_std_7', 'flow_lag1', 'WOY', 'weights_rainfall', 'weights_flow'] +
                                  [f'rainfall_rolling_avg_{n}' for n in range(2, 11)]),
        ('cat', OneHotEncoder(drop='first'), ['season', 'holiday'])
    ])

X_train_scaled = preprocessor.fit_transform(X_train)
X_test_scaled = preprocessor.transform(X_test)

# ARIMA model
p, d, q = 1, 0, 1  # ARIMA parameters, visually inspected PACF and ACF
model = ARIMA(train, order=(p, d, q), exog=X_train_scaled)
model_fit = model.fit()
predictions = model_fit.predict(start=len(train), end=len(train) + len(test) - 1, exog=X_test_scaled)

# Model performance metrics
rmse = sqrt(mean_squared_error(test, predictions))
mae = mean_absolute_error(test, predictions)
r2 = r2_score(test, predictions)

print(f'RMSE: {rmse:.2f}, MAE: {mae:.2f}, RÂ²: {r2:.2f}')

# Plot Observed vs. Predicted
plt.figure(figsize=(12, 6))
dates_test = data.index[size:]  # Corresponding dates for the test set
dates_pred = data.index[size:len(train) + len(predictions)]  # Dates for predictions

plt.plot(dates_test, test, label='Observed', color='blue')
plt.plot(dates_pred, predictions, label='Predicted', color='red')
plt.title('ARIMAX with Hurdle-Gamma and GEV weights')
plt.xlabel('Date')
plt.ylabel('Flow (million gallons per day)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from prophet import Prophet
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Load and preprocess data
data = pd.read_csv('plant_flow_data.csv')
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

us_holidays = holidays.US(years=data.index.year.unique())
data['holiday'] = data.index.to_series().apply(lambda x: 1 if x in us_holidays else 0)

# Rolling average rainfall predictors
for n in range(2, 11):
    data[f'rainfall_rolling_avg_{n}'] = data['rainfall'].rolling(window=n).mean()

# Prepare data for Prophet
prophet_data = data[['flow', 'rainfall', 'season', 'ADD']].reset_index()
prophet_data.columns = ['ds', 'y', 'rainfall', 'season', 'ADD']

# Transform 'season' to a numerical feature using LabelEncoder
label_encoder = LabelEncoder()
prophet_data['season'] = label_encoder.fit_transform(prophet_data['season'])

# Split data into train and test sets
train_size = int(len(prophet_data) * 0.8)
train, test = prophet_data[:train_size], prophet_data[train_size:]

# Define and fit the Prophet model, adding additional regressors
model = Prophet()
model.add_regressor('rainfall')
model.add_regressor('season')
model.add_regressor('ADD')  # ADD: antecedent dry days, must be precomputed in the dataset
model.fit(train)

# Create a future DataFrame and set values for the regressors
future = model.make_future_dataframe(periods=len(test), freq='D')
future['rainfall'] = np.tile(train['rainfall'].values[-1], len(future))  # Placeholder example
future['ADD'] = np.tile(train['ADD'].values[-1], len(future))            # Placeholder example
# Adjust 'season' to match the future length by tiling
future['season'] = np.tile(train['season'].values, int(np.ceil(len(future) / len(train))))[:len(future)]

# Make predictions with the model
forecast = model.predict(future)

# Model evaluation
y_true = test['y'].values
y_pred = forecast['yhat'].values[-len(test):]

mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))

print(f'MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.2f}')

# Plot the forecast
fig = model.plot(forecast)
plt.title('Flow Forecast using Prophet')
plt.xlabel('Time')
plt.ylabel('Flow (million gallons per day)')
plt.grid(False)
plt.show()

# Residuals analysis plot
residuals = y_true - y_pred
plt.figure(figsize=(10, 6))
plt.plot(test['ds'], residuals, linestyle='-', color='b', label='Residuals')
plt.axhline(0, color='red', linestyle='--', label='Zero Line')
plt.title('Residuals Analysis')
plt.xlabel('Time')
plt.ylabel('Residuals')
plt.legend()
plt.grid(False)
plt.show()